{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing and importing nltk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "#dowloading data/corpus\n",
    "#3\n",
    "#importing brown corpus\n",
    "from nltk.corpus import brown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#listing\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'were',\n",
       " 'thirty-eight',\n",
       " 'patients',\n",
       " 'on',\n",
       " 'the',\n",
       " 'bus',\n",
       " 'the',\n",
       " 'morning',\n",
       " 'I',\n",
       " 'left',\n",
       " 'for',\n",
       " 'Hanover',\n",
       " ',',\n",
       " 'most',\n",
       " 'of',\n",
       " 'them',\n",
       " 'disturbed',\n",
       " 'and',\n",
       " 'hallucinating']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download data\n",
    "brown.words(categories='adventure')[:20]\n",
    "brown.words(categories='mystery')[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.2\n",
    "#importing inaugural corpus\n",
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listing and download\n",
    "inaugural.fileids()\n",
    "words=inaugural.words(fileids = '2009-Obama.txt')[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'fellow', 'citizens', ':', 'I', 'stand', 'here', 'today', 'humbled', 'by', 'the', 'task', 'before', 'us', ',', 'grateful', 'for', 'the', 'trust', 'you', 'have', 'bestowed', ',', 'mindful', 'of', 'the', 'sacrifices', 'borne', 'by', 'our', 'ancestors', '.', 'I', 'thank', 'President', 'Bush', 'for', 'his', 'service', 'to', 'our', 'nation', ',', 'as', 'well', 'as', 'the', 'generosity', 'and', 'cooperation', 'he', 'has', 'shown', 'throughout', 'this', 'transition', '.', 'Forty', '-', 'four', 'Americans', 'have', 'now', 'taken', 'the', 'presidential', 'oath', '.', 'The', 'words', 'have', 'been', 'spoken', 'during', 'rising', 'tides', 'of', 'prosperity', 'and', 'the', 'still', 'waters', 'of', 'peace', '.', 'Yet', ',', 'every', 'so', 'often', 'the', 'oath', 'is', 'taken', 'amidst', 'gathering', 'clouds', 'and', 'raging', 'storms', '.', 'At', 'these', 'moments', ',', 'America', 'has', 'carried', 'on', 'not', 'simply', 'because', 'of', 'the', 'skill', 'or', 'vision', 'of', 'those', 'in', 'high', 'office', ',', 'but', 'because', 'We', 'the', 'People', 'have', 'remained', 'faithful', 'to', 'the', 'ideals', 'of', 'our', 'forbearers', ',', 'and', 'true', 'to', 'our', 'founding', 'documents', '.', 'So', 'it', 'has', 'been', '.', 'So', 'it', 'must', 'be', 'with', 'this', 'generation', 'of', 'Americans', '.', 'That', 'we', 'are', 'in', 'the', 'midst', 'of', 'crisis', 'is', 'now', 'well', 'understood', '.', 'Our', 'nation', 'is', 'at', 'war', ',', 'against', 'a', 'far', '-', 'reaching', 'network', 'of', 'violence', 'and', 'hatred', '.', 'Our', 'economy', 'is', 'badly', 'weakened', ',', 'a', 'consequence', 'of', 'greed', 'and', 'irresponsibility', 'on', 'the', 'part', 'of', 'some', ',', 'but', 'also', 'our', 'collective', 'failure', 'to', 'make', 'hard', 'choices', 'and', 'prepare', 'the', 'nation', 'for', 'a', 'new', 'age', '.', 'Homes', 'have', 'been', 'lost', ';', 'jobs', 'shed', ';', 'businesses', 'shuttered', '.', 'Our', 'health', 'care', 'is', 'too', 'costly', ';', 'our', 'schools', 'fail', 'too', 'many', ';', 'and', 'each', 'day', 'brings', 'further', 'evidence', 'that', 'the', 'ways', 'we', 'use', 'energy', 'strengthen', 'our', 'adversaries', 'and', 'threaten', 'our', 'planet', '.', 'These', 'are', 'the', 'indicators', 'of', 'crisis', ',', 'subject', 'to', 'data', 'and', 'statistics', '.', 'Less', 'measurable', 'but', 'no', 'less', 'profound', 'is', 'a', 'sapping', 'of', 'confidence', 'across', 'our', 'land', '--', 'a', 'nagging', 'fear', 'that', 'America', \"'\", 's', 'decline', 'is', 'inevitable', ',', 'that', 'the', 'next', 'generation', 'must', 'lower', 'its', 'sights', '.', 'Today', 'I', 'say', 'to', 'you', 'that', 'the', 'challenges', 'we', 'face', 'are', 'real', '.', 'They', 'are', 'serious', 'and', 'they', 'are', 'many', '.', 'They', 'will', 'not', 'be', 'met', 'easily', 'or', 'in', 'a', 'short', 'span', 'of', 'time', '.', 'But', 'know', 'this', ',', 'America', '--', 'they', 'will', 'be', 'met', '.', 'On', 'this', 'day', ',', 'we', 'gather', 'because', 'we', 'have', 'chosen', 'hope', 'over', 'fear', ',', 'unity', 'of', 'purpose', 'over', 'conflict', 'and', 'discord', '.', 'On', 'this', 'day', ',', 'we', 'come', 'to', 'proclaim', 'an', 'end', 'to', 'the', 'petty', 'grievances', 'and', 'false', 'promises', ',', 'the', 'recriminations', 'and', 'worn', '-', 'out', 'dogmas', 'that', 'for', 'far', 'too', 'long', 'have', 'strangled', 'our', 'politics', '.', 'We', 'remain', 'a', 'young', 'nation', ',', 'but', 'in', 'the', 'words', 'of', 'Scripture', ',', 'the', 'time', 'has', 'come', 'to', 'set', 'aside', 'childish', 'things', '.', 'The', 'time', 'has', 'come', 'to', 'reaffirm', 'our', 'enduring', 'spirit', ';', 'to', 'choose', 'our', 'better', 'history', ';', 'to', 'carry', 'forward', 'that', 'precious', 'gift', ',', 'that', 'noble', 'idea', ',', 'passed', 'on', 'from', 'generation', 'to', 'generation', ':', 'the', 'God', '-', 'given', 'promise', 'that', 'all', 'are', 'equal', ',', 'all', 'are', 'free', ',', 'and', 'all', 'deserve', 'a', 'chance', 'to', 'pursue', 'their']\n"
     ]
    }
   ],
   "source": [
    "print(words)\n",
    "#frequency of words in one speech\n",
    "#table of word frequencies in different speeches\n",
    "#How many three leteer words\n",
    "#chinese text segmentation\n",
    "#plagiarism tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq={}\n",
    "for word in words:\n",
    "    if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "    else:\n",
    "            word_freq[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'My': 1,\n",
       " 'fellow': 1,\n",
       " 'citizens': 1,\n",
       " ':': 2,\n",
       " 'I': 3,\n",
       " 'stand': 1,\n",
       " 'here': 1,\n",
       " 'today': 1,\n",
       " 'humbled': 1,\n",
       " 'by': 2,\n",
       " 'the': 22,\n",
       " 'task': 1,\n",
       " 'before': 1,\n",
       " 'us': 1,\n",
       " ',': 23,\n",
       " 'grateful': 1,\n",
       " 'for': 4,\n",
       " 'trust': 1,\n",
       " 'you': 2,\n",
       " 'have': 7,\n",
       " 'bestowed': 1,\n",
       " 'mindful': 1,\n",
       " 'of': 16,\n",
       " 'sacrifices': 1,\n",
       " 'borne': 1,\n",
       " 'our': 12,\n",
       " 'ancestors': 1,\n",
       " '.': 22,\n",
       " 'thank': 1,\n",
       " 'President': 1,\n",
       " 'Bush': 1,\n",
       " 'his': 1,\n",
       " 'service': 1,\n",
       " 'to': 14,\n",
       " 'nation': 4,\n",
       " 'as': 2,\n",
       " 'well': 2,\n",
       " 'generosity': 1,\n",
       " 'and': 15,\n",
       " 'cooperation': 1,\n",
       " 'he': 1,\n",
       " 'has': 5,\n",
       " 'shown': 1,\n",
       " 'throughout': 1,\n",
       " 'this': 5,\n",
       " 'transition': 1,\n",
       " 'Forty': 1,\n",
       " '-': 4,\n",
       " 'four': 1,\n",
       " 'Americans': 2,\n",
       " 'now': 2,\n",
       " 'taken': 2,\n",
       " 'presidential': 1,\n",
       " 'oath': 2,\n",
       " 'The': 2,\n",
       " 'words': 2,\n",
       " 'been': 3,\n",
       " 'spoken': 1,\n",
       " 'during': 1,\n",
       " 'rising': 1,\n",
       " 'tides': 1,\n",
       " 'prosperity': 1,\n",
       " 'still': 1,\n",
       " 'waters': 1,\n",
       " 'peace': 1,\n",
       " 'Yet': 1,\n",
       " 'every': 1,\n",
       " 'so': 1,\n",
       " 'often': 1,\n",
       " 'is': 7,\n",
       " 'amidst': 1,\n",
       " 'gathering': 1,\n",
       " 'clouds': 1,\n",
       " 'raging': 1,\n",
       " 'storms': 1,\n",
       " 'At': 1,\n",
       " 'these': 1,\n",
       " 'moments': 1,\n",
       " 'America': 3,\n",
       " 'carried': 1,\n",
       " 'on': 3,\n",
       " 'not': 2,\n",
       " 'simply': 1,\n",
       " 'because': 3,\n",
       " 'skill': 1,\n",
       " 'or': 2,\n",
       " 'vision': 1,\n",
       " 'those': 1,\n",
       " 'in': 4,\n",
       " 'high': 1,\n",
       " 'office': 1,\n",
       " 'but': 4,\n",
       " 'We': 2,\n",
       " 'People': 1,\n",
       " 'remained': 1,\n",
       " 'faithful': 1,\n",
       " 'ideals': 1,\n",
       " 'forbearers': 1,\n",
       " 'true': 1,\n",
       " 'founding': 1,\n",
       " 'documents': 1,\n",
       " 'So': 2,\n",
       " 'it': 2,\n",
       " 'must': 2,\n",
       " 'be': 3,\n",
       " 'with': 1,\n",
       " 'generation': 4,\n",
       " 'That': 1,\n",
       " 'we': 6,\n",
       " 'are': 7,\n",
       " 'midst': 1,\n",
       " 'crisis': 2,\n",
       " 'understood': 1,\n",
       " 'Our': 3,\n",
       " 'at': 1,\n",
       " 'war': 1,\n",
       " 'against': 1,\n",
       " 'a': 8,\n",
       " 'far': 2,\n",
       " 'reaching': 1,\n",
       " 'network': 1,\n",
       " 'violence': 1,\n",
       " 'hatred': 1,\n",
       " 'economy': 1,\n",
       " 'badly': 1,\n",
       " 'weakened': 1,\n",
       " 'consequence': 1,\n",
       " 'greed': 1,\n",
       " 'irresponsibility': 1,\n",
       " 'part': 1,\n",
       " 'some': 1,\n",
       " 'also': 1,\n",
       " 'collective': 1,\n",
       " 'failure': 1,\n",
       " 'make': 1,\n",
       " 'hard': 1,\n",
       " 'choices': 1,\n",
       " 'prepare': 1,\n",
       " 'new': 1,\n",
       " 'age': 1,\n",
       " 'Homes': 1,\n",
       " 'lost': 1,\n",
       " ';': 6,\n",
       " 'jobs': 1,\n",
       " 'shed': 1,\n",
       " 'businesses': 1,\n",
       " 'shuttered': 1,\n",
       " 'health': 1,\n",
       " 'care': 1,\n",
       " 'too': 3,\n",
       " 'costly': 1,\n",
       " 'schools': 1,\n",
       " 'fail': 1,\n",
       " 'many': 2,\n",
       " 'each': 1,\n",
       " 'day': 3,\n",
       " 'brings': 1,\n",
       " 'further': 1,\n",
       " 'evidence': 1,\n",
       " 'that': 8,\n",
       " 'ways': 1,\n",
       " 'use': 1,\n",
       " 'energy': 1,\n",
       " 'strengthen': 1,\n",
       " 'adversaries': 1,\n",
       " 'threaten': 1,\n",
       " 'planet': 1,\n",
       " 'These': 1,\n",
       " 'indicators': 1,\n",
       " 'subject': 1,\n",
       " 'data': 1,\n",
       " 'statistics': 1,\n",
       " 'Less': 1,\n",
       " 'measurable': 1,\n",
       " 'no': 1,\n",
       " 'less': 1,\n",
       " 'profound': 1,\n",
       " 'sapping': 1,\n",
       " 'confidence': 1,\n",
       " 'across': 1,\n",
       " 'land': 1,\n",
       " '--': 2,\n",
       " 'nagging': 1,\n",
       " 'fear': 2,\n",
       " \"'\": 1,\n",
       " 's': 1,\n",
       " 'decline': 1,\n",
       " 'inevitable': 1,\n",
       " 'next': 1,\n",
       " 'lower': 1,\n",
       " 'its': 1,\n",
       " 'sights': 1,\n",
       " 'Today': 1,\n",
       " 'say': 1,\n",
       " 'challenges': 1,\n",
       " 'face': 1,\n",
       " 'real': 1,\n",
       " 'They': 2,\n",
       " 'serious': 1,\n",
       " 'they': 2,\n",
       " 'will': 2,\n",
       " 'met': 2,\n",
       " 'easily': 1,\n",
       " 'short': 1,\n",
       " 'span': 1,\n",
       " 'time': 3,\n",
       " 'But': 1,\n",
       " 'know': 1,\n",
       " 'On': 2,\n",
       " 'gather': 1,\n",
       " 'chosen': 1,\n",
       " 'hope': 1,\n",
       " 'over': 2,\n",
       " 'unity': 1,\n",
       " 'purpose': 1,\n",
       " 'conflict': 1,\n",
       " 'discord': 1,\n",
       " 'come': 3,\n",
       " 'proclaim': 1,\n",
       " 'an': 1,\n",
       " 'end': 1,\n",
       " 'petty': 1,\n",
       " 'grievances': 1,\n",
       " 'false': 1,\n",
       " 'promises': 1,\n",
       " 'recriminations': 1,\n",
       " 'worn': 1,\n",
       " 'out': 1,\n",
       " 'dogmas': 1,\n",
       " 'long': 1,\n",
       " 'strangled': 1,\n",
       " 'politics': 1,\n",
       " 'remain': 1,\n",
       " 'young': 1,\n",
       " 'Scripture': 1,\n",
       " 'set': 1,\n",
       " 'aside': 1,\n",
       " 'childish': 1,\n",
       " 'things': 1,\n",
       " 'reaffirm': 1,\n",
       " 'enduring': 1,\n",
       " 'spirit': 1,\n",
       " 'choose': 1,\n",
       " 'better': 1,\n",
       " 'history': 1,\n",
       " 'carry': 1,\n",
       " 'forward': 1,\n",
       " 'precious': 1,\n",
       " 'gift': 1,\n",
       " 'noble': 1,\n",
       " 'idea': 1,\n",
       " 'passed': 1,\n",
       " 'from': 1,\n",
       " 'God': 1,\n",
       " 'given': 1,\n",
       " 'promise': 1,\n",
       " 'all': 3,\n",
       " 'equal': 1,\n",
       " 'free': 1,\n",
       " 'deserve': 1,\n",
       " 'chance': 1,\n",
       " 'pursue': 1,\n",
       " 'their': 1}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After stopword removal:  ['My', 'fellow', 'citizens', ':', 'I', 'stand', 'today', 'humbled', 'task', 'us', ',', 'grateful', 'trust', 'bestowed', ',', 'mindful', 'sacrifices', 'borne', 'ancestors', '.', 'I', 'thank', 'President', 'Bush', 'service', 'nation', ',', 'well', 'generosity', 'cooperation', 'shown', 'throughout', 'transition', '.', 'Forty', '-', 'four', 'Americans', 'taken', 'presidential', 'oath', '.', 'The', 'words', 'spoken', 'rising', 'tides', 'prosperity', 'still', 'waters', 'peace', '.', 'Yet', ',', 'every', 'often', 'oath', 'taken', 'amidst', 'gathering', 'clouds', 'raging', 'storms', '.', 'At', 'moments', ',', 'America', 'carried', 'simply', 'skill', 'vision', 'high', 'office', ',', 'We', 'People', 'remained', 'faithful', 'ideals', 'forbearers', ',', 'true', 'founding', 'documents', '.', 'So', '.', 'So', 'must', 'generation', 'Americans', '.', 'That', 'midst', 'crisis', 'well', 'understood', '.', 'Our', 'nation', 'war', ',', 'far', '-', 'reaching', 'network', 'violence', 'hatred', '.', 'Our', 'economy', 'badly', 'weakened', ',', 'consequence', 'greed', 'irresponsibility', 'part', ',', 'also', 'collective', 'failure', 'make', 'hard', 'choices', 'prepare', 'nation', 'new', 'age', '.', 'Homes', 'lost', ';', 'jobs', 'shed', ';', 'businesses', 'shuttered', '.', 'Our', 'health', 'care', 'costly', ';', 'schools', 'fail', 'many', ';', 'day', 'brings', 'evidence', 'ways', 'use', 'energy', 'strengthen', 'adversaries', 'threaten', 'planet', '.', 'These', 'indicators', 'crisis', ',', 'subject', 'data', 'statistics', '.', 'Less', 'measurable', 'less', 'profound', 'sapping', 'confidence', 'across', 'land', '--', 'nagging', 'fear', 'America', \"'\", 'decline', 'inevitable', ',', 'next', 'generation', 'must', 'lower', 'sights', '.', 'Today', 'I', 'say', 'challenges', 'face', 'real', '.', 'They', 'serious', 'many', '.', 'They', 'met', 'easily', 'short', 'span', 'time', '.', 'But', 'know', ',', 'America', '--', 'met', '.', 'On', 'day', ',', 'gather', 'chosen', 'hope', 'fear', ',', 'unity', 'purpose', 'conflict', 'discord', '.', 'On', 'day', ',', 'come', 'proclaim', 'end', 'petty', 'grievances', 'false', 'promises', ',', 'recriminations', 'worn', '-', 'dogmas', 'far', 'long', 'strangled', 'politics', '.', 'We', 'remain', 'young', 'nation', ',', 'words', 'Scripture', ',', 'time', 'come', 'set', 'aside', 'childish', 'things', '.', 'The', 'time', 'come', 'reaffirm', 'enduring', 'spirit', ';', 'choose', 'better', 'history', ';', 'carry', 'forward', 'precious', 'gift', ',', 'noble', 'idea', ',', 'passed', 'generation', 'generation', ':', 'God', '-', 'given', 'promise', 'equal', ',', 'free', ',', 'deserve', 'chance', 'pursue']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "filtered_sentence = [w for w in words if not w in stop_words]\n",
    "print(\"After stopword removal: \", filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq={}\n",
    "for word in filtered_sentence:\n",
    "    if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "    else:\n",
    "            word_freq[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist=FreqDist(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The five most frequent words are: \n",
      "nation: 4\n",
      "generation: 4\n",
      "I: 3\n",
      "America: 3\n",
      "Our: 3\n"
     ]
    }
   ],
   "source": [
    "#finding top five most frequent words\n",
    "import re\n",
    "max_dict = {}\n",
    "while len(max_dict) < 5:\n",
    "    max_val = 0\n",
    "    for key in word_freq:\n",
    "        if max_val < word_freq[key] and re.match(r'[A-Za-z]+',key) and key not in max_dict:\n",
    "            max_key = key\n",
    "            max_val = word_freq[key]\n",
    "    max_dict[max_key] = max_val\n",
    "    \n",
    "print(\"The five most frequent words are: \")\n",
    "for key in max_dict:\n",
    "    print(key+\":\",max_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 210 samples and 297 outcomes>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-fb870e6e71b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfdist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m54\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcumulative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'plot'"
     ]
    }
   ],
   "source": [
    "#3.3\n",
    "print(fdist)\n",
    "import matplotlib as plt\n",
    "plt.plot(54,cumulative=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown,inaugural,genesis\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.4 cfd and plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
